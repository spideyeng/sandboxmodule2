{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5249624e",
   "metadata": {},
   "source": [
    "# To Test Extraction of Brazil Olist datasets from Kaggle using Python \n",
    "_run this using base Python environment_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f312d",
   "metadata": {},
   "source": [
    "## Authenticate and Create the API Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a62be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Create the API object\n",
    "api = KaggleApi()\n",
    "\n",
    "# Authenticate using kaggle.json in ~/.kaggle/\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247031b",
   "metadata": {},
   "source": [
    "## List all files in the Brazil Olist dataset - Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32022a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files:\n",
      "olist_customers_dataset.csv\n",
      "olist_geolocation_dataset.csv\n",
      "olist_order_items_dataset.csv\n",
      "olist_order_payments_dataset.csv\n",
      "olist_order_reviews_dataset.csv\n",
      "olist_orders_dataset.csv\n",
      "olist_products_dataset.csv\n",
      "olist_sellers_dataset.csv\n",
      "product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# List files in the dataset\n",
    "dataset = \"olistbr/brazilian-ecommerce\"\n",
    "files = api.dataset_list_files(dataset).files\n",
    "\n",
    "print(\"Available files:\")\n",
    "for f in files:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d43f7b",
   "metadata": {},
   "source": [
    "## List all files in the Brazil Olist dataset - Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3a598c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olist_customers_dataset.csv\n",
      "olist_geolocation_dataset.csv\n",
      "olist_order_items_dataset.csv\n",
      "olist_order_payments_dataset.csv\n",
      "olist_order_reviews_dataset.csv\n",
      "olist_orders_dataset.csv\n",
      "olist_products_dataset.csv\n",
      "olist_sellers_dataset.csv\n",
      "product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "files = api.dataset_list_files(\"olistbr/brazilian-ecommerce\").files\n",
    "for f in files:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad6f82",
   "metadata": {},
   "source": [
    "## Loading olist_orders_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2cef927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "\n",
      "  order_estimated_delivery_date  \n",
      "0           2017-10-18 00:00:00  \n",
      "1           2018-08-13 00:00:00  \n",
      "2           2018-09-04 00:00:00  \n",
      "3           2017-12-15 00:00:00  \n",
      "4           2018-02-26 00:00:00  \n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Choose the file you want to load from the dataset\n",
    "file_path = \"olist_orders_dataset.csv\"\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df_orders = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"olistbr/brazilian-ecommerce\",\n",
    "    file_path\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\")\n",
    "print(df_orders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c844f7",
   "metadata": {},
   "source": [
    "### Dataframe Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f56655c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99441 entries, 0 to 99440\n",
      "Data columns (total 8 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   order_id                       99441 non-null  object\n",
      " 1   customer_id                    99441 non-null  object\n",
      " 2   order_status                   99441 non-null  object\n",
      " 3   order_purchase_timestamp       99441 non-null  object\n",
      " 4   order_approved_at              99281 non-null  object\n",
      " 5   order_delivered_carrier_date   97658 non-null  object\n",
      " 6   order_delivered_customer_date  96476 non-null  object\n",
      " 7   order_estimated_delivery_date  99441 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 6.1+ MB\n",
      "None\n",
      "                                order_id                       customer_id  \\\n",
      "count                              99441                             99441   \n",
      "unique                             99441                             99441   \n",
      "top     e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "freq                                   1                                 1   \n",
      "\n",
      "       order_status order_purchase_timestamp    order_approved_at  \\\n",
      "count         99441                    99441                99281   \n",
      "unique            8                    98875                90733   \n",
      "top       delivered      2018-08-02 12:06:07  2018-02-27 04:31:10   \n",
      "freq          96478                        3                    9   \n",
      "\n",
      "       order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "count                         97658                         96476   \n",
      "unique                        81018                         95664   \n",
      "top             2018-05-09 15:48:00           2018-05-14 20:02:44   \n",
      "freq                             47                             3   \n",
      "\n",
      "       order_estimated_delivery_date  \n",
      "count                          99441  \n",
      "unique                           459  \n",
      "top              2017-12-20 00:00:00  \n",
      "freq                             522  \n"
     ]
    }
   ],
   "source": [
    "df_orders\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(df_orders.info())\n",
    "print(df_orders.describe())\n",
    "#print(df_orders.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6e70b",
   "metadata": {},
   "source": [
    "## Loading olist_customers_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b2821e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      "                        customer_id                customer_unique_id  \\\n",
      "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
      "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
      "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
      "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
      "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city customer_state  \n",
      "0                     14409                 franca             SP  \n",
      "1                      9790  sao bernardo do campo             SP  \n",
      "2                      1151              sao paulo             SP  \n",
      "3                      8775        mogi das cruzes             SP  \n",
      "4                     13056               campinas             SP  \n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Choose the file you want to load from the dataset\n",
    "file_path = \"olist_customers_dataset.csv\"\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df_customers = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"olistbr/brazilian-ecommerce\",\n",
    "    file_path\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\")\n",
    "print(df_customers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f2d28",
   "metadata": {},
   "source": [
    "# Iterating Over Multiple Files - Here’s how you can loop through all the CSVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be1c83d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_orders_dataset.csv, shape: (99441, 8)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=olist_order_items_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.37M/6.37M [00:01<00:00, 5.06MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of olist_order_items_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_order_items_dataset.csv, shape: (112650, 7)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=olist_products_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.02M/1.02M [00:00<00:00, 1.11MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of olist_products_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_products_dataset.csv, shape: (32951, 9)\n",
      "Loaded olist_customers_dataset.csv, shape: (99441, 5)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=olist_sellers_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171k/171k [00:00<00:00, 287kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_sellers_dataset.csv, shape: (3095, 4)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=olist_geolocation_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14.9M/14.9M [00:01<00:00, 9.29MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of olist_geolocation_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_geolocation_dataset.csv, shape: (1000163, 5)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=olist_order_reviews_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.42M/6.42M [00:00<00:00, 7.37MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of olist_order_reviews_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_order_reviews_dataset.csv, shape: (99224, 7)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=olist_order_payments_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.49M/2.49M [00:01<00:00, 2.20MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of olist_order_payments_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded olist_order_payments_dataset.csv, shape: (103886, 5)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce?dataset_version_number=2&file_name=product_category_name_translation.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.55k/2.55k [00:00<00:00, 3.06MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded product_category_name_translation.csv, shape: (71, 2)\n",
      "First 5 records from orders dataset:\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "\n",
      "  order_estimated_delivery_date  \n",
      "0           2017-10-18 00:00:00  \n",
      "1           2018-08-13 00:00:00  \n",
      "2           2018-09-04 00:00:00  \n",
      "3           2017-12-15 00:00:00  \n",
      "4           2018-02-26 00:00:00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# List of all 9 CSV files in the dataset\n",
    "files = [\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\"\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Iterate and load each file\n",
    "for file_path in files:\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"olistbr/brazilian-ecommerce\",\n",
    "        file_path\n",
    "    )\n",
    "    dfs[file_path] = df\n",
    "    print(f\"Loaded {file_path}, shape: {df.shape}\")\n",
    "\n",
    "# Example: Access one DataFrame\n",
    "print(\"First 5 records from orders dataset:\")\n",
    "print(dfs[\"olist_orders_dataset.csv\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ad295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records from customers dataset:\n",
      "                        customer_id                customer_unique_id  \\\n",
      "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
      "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
      "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
      "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
      "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city customer_state  \n",
      "0                     14409                 franca             SP  \n",
      "1                      9790  sao bernardo do campo             SP  \n",
      "2                      1151              sao paulo             SP  \n",
      "3                      8775        mogi das cruzes             SP  \n",
      "4                     13056               campinas             SP  \n"
     ]
    }
   ],
   "source": [
    "# Example: Access DataFrame for Customers\n",
    "print(\"First 5 records from customers dataset:\")\n",
    "print(dfs[\"olist_customers_dataset.csv\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7719d89",
   "metadata": {},
   "source": [
    "# Full Workflow: Kaggle → Pandas → BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6cae7",
   "metadata": {},
   "source": [
    "##  Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2b35145",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Using cached google_cloud_bigquery-3.38.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: kagglehub[pandas-datasets] in /home/engpookw/miniconda3/lib/python3.13/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in /home/engpookw/miniconda3/lib/python3.13/site-packages (from kagglehub[pandas-datasets]) (25.0)\n",
      "Requirement already satisfied: pyyaml in /home/engpookw/miniconda3/lib/python3.13/site-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/engpookw/miniconda3/lib/python3.13/site-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /home/engpookw/miniconda3/lib/python3.13/site-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
      "Requirement already satisfied: pandas in /home/engpookw/miniconda3/lib/python3.13/site-packages (from kagglehub[pandas-datasets]) (2.3.3)\n",
      "Collecting google-api-core<3.0.0,>=2.11.1 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-cloud-bigquery)\n",
      "  Using cached google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1 (from google-cloud-bigquery)\n",
      "  Using cached google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery)\n",
      "  Using cached google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (6.33.1)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n",
      "  Using cached grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-crc32c<2.0.0,>=1.0.0 (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery)\n",
      "  Downloading google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests->kagglehub[pandas-datasets]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests->kagglehub[pandas-datasets]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests->kagglehub[pandas-datasets]) (2025.11.12)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas->kagglehub[pandas-datasets]) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
      "Using cached google_cloud_bigquery-3.38.0-py3-none-any.whl (259 kB)\n",
      "Using cached google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Using cached google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Using cached cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Using cached google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Downloading google_crc32c-1.7.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: pyasn1, pyarrow, proto-plus, grpcio, googleapis-common-protos, google-crc32c, cachetools, rsa, pyasn1-modules, grpcio-status, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [google-cloud-bigquery]le-cloud-bigquery]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cachetools-6.2.2 google-api-core-2.28.1 google-auth-2.43.0 google-cloud-bigquery-3.38.0 google-cloud-core-2.5.0 google-crc32c-1.7.1 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 grpcio-1.76.0 grpcio-status-1.76.0 proto-plus-1.26.1 pyarrow-22.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub[pandas-datasets] google-cloud-bigquery pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c83e860e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-gbq\n",
      "  Using cached pandas_gbq-0.31.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (78.1.1)\n",
      "Collecting db-dtypes<2.0.0,>=1.0.4 (from pandas-gbq)\n",
      "  Using cached db_dtypes-1.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (2.3.3)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (22.0.0)\n",
      "Collecting pydata-google-auth>=1.5.0 (from pandas-gbq)\n",
      "  Using cached pydata_google_auth-1.9.1-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: psutil>=5.9.8 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (7.0.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.15.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (2.28.1)\n",
      "Requirement already satisfied: google-auth>=2.14.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (2.43.0)\n",
      "Collecting google-auth-oauthlib>=0.7.0 (from pandas-gbq)\n",
      "  Using cached google_auth_oauthlib-1.2.3-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0,>=3.20.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (3.38.0)\n",
      "Requirement already satisfied: packaging>=22.0.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas-gbq) (25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->pandas-gbq) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->pandas-gbq) (6.33.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->pandas-gbq) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.15.0->pandas-gbq) (2.32.4)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-auth>=2.14.1->pandas-gbq) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-auth>=2.14.1->pandas-gbq) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-auth>=2.14.1->pandas-gbq) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (2.9.0.post0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (1.76.0)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.0.0 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (1.7.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery<4.0.0,>=3.20.0->pandas-gbq) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.15.0->pandas-gbq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.15.0->pandas-gbq) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.15.0->pandas-gbq) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.15.0->pandas-gbq) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=2.14.1->pandas-gbq) (0.6.1)\n",
      "Collecting google-auth>=2.14.1 (from pandas-gbq)\n",
      "  Using cached google_auth-2.41.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib>=0.7.0->pandas-gbq)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas>=1.1.4->pandas-gbq) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/engpookw/miniconda3/lib/python3.13/site-packages (from pandas>=1.1.4->pandas-gbq) (2025.2)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas-gbq)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Using cached pandas_gbq-0.31.0-py3-none-any.whl (46 kB)\n",
      "Using cached db_dtypes-1.4.4-py3-none-any.whl (18 kB)\n",
      "Using cached google_auth_oauthlib-1.2.3-py3-none-any.whl (19 kB)\n",
      "Using cached google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\n",
      "Using cached pydata_google_auth-1.9.1-py2.py3-none-any.whl (15 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, google-auth, google-auth-oauthlib, db-dtypes, pydata-google-auth, pandas-gbq\n",
      "\u001b[2K  Attempting uninstall: google-auth\n",
      "\u001b[2K    Found existing installation: google-auth 2.43.0\n",
      "\u001b[2K    Uninstalling google-auth-2.43.0:\n",
      "\u001b[2K      Successfully uninstalled google-auth-2.43.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [pandas-gbq]7\u001b[0m [pandas-gbq]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed db-dtypes-1.4.4 google-auth-2.41.1 google-auth-oauthlib-1.2.3 oauthlib-3.3.1 pandas-gbq-0.31.0 pydata-google-auth-1.9.1 requests-oauthlib-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas-gbq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689fb3a",
   "metadata": {},
   "source": [
    "## Create the Dataset Before Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4520485a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset my-module-2-group-project.brazil_olist_data is ready.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"/home/engpookw/secure/my-module-2-group-project-d6d1e6f07169.json\"\n",
    ")\n",
    "client = bigquery.Client(credentials=credentials, project=\"my-module-2-group-project\")\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "dataset_id = f\"{client.project}.brazil_olist_data\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"   # or \"asia-southeast1\" for Singapore region\n",
    "\n",
    "try:\n",
    "    client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"✅ Dataset {dataset_id} is ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b398c",
   "metadata": {},
   "source": [
    "## Loading into BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbe056",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "*Sample Code*\n",
    "```python\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Authenticate with BigQuery\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"path/to/service_account.json\"\n",
    ")\n",
    "client = bigquery.Client(credentials=credentials, project=\"your-gcp-project-id\")\n",
    "\n",
    "# List of all 9 CSV files in the dataset\n",
    "files = [\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\"\n",
    "]\n",
    "\n",
    "# Dataset ID in BigQuery\n",
    "dataset_id = \"your_dataset\"\n",
    "\n",
    "# Iterate through files, load into Pandas, and upload to BigQuery\n",
    "for file_path in files:\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    \n",
    "    # Load CSV into Pandas DataFrame\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"olistbr/brazilian-ecommerce\",\n",
    "        file_path\n",
    "    )\n",
    "    \n",
    "    # Define BigQuery table name (strip .csv)\n",
    "    table_id = f\"your-gcp-project-id.{dataset_id}.{file_path.replace('.csv','')}\"\n",
    "    \n",
    "    # Upload DataFrame to BigQuery\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # Wait for job to complete\n",
    "    \n",
    "    print(f\"✅ Uploaded {file_path} to {table_id}, shape: {df.shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e99e600f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading olist_orders_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_orders_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_orders_dataset, shape: (99441, 8)\n",
      "Loading olist_order_items_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_order_items_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_order_items_dataset, shape: (112650, 7)\n",
      "Loading olist_products_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_products_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_products_dataset, shape: (32951, 9)\n",
      "Loading olist_customers_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_customers_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_customers_dataset, shape: (99441, 5)\n",
      "Loading olist_sellers_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_sellers_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_sellers_dataset, shape: (3095, 4)\n",
      "Loading olist_geolocation_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_geolocation_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_geolocation_dataset, shape: (1000163, 5)\n",
      "Loading olist_order_reviews_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_order_reviews_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_order_reviews_dataset, shape: (99224, 7)\n",
      "Loading olist_order_payments_dataset.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded olist_order_payments_dataset.csv to my-module-2-group-project.brazil_olist_data.olist_order_payments_dataset, shape: (103886, 5)\n",
      "Loading product_category_name_translation.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engpookw/miniconda3/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded product_category_name_translation.csv to my-module-2-group-project.brazil_olist_data.product_category_name_translation, shape: (71, 2)\n",
      "All files have been uploaded to BigQuery.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Authenticate with BigQuery\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"/home/engpookw/secure/my-module-2-group-project-d6d1e6f07169.json\"\n",
    ")\n",
    "client = bigquery.Client(credentials=credentials, project=\"my-module-2-group-project\")\n",
    "\n",
    "# List of all 9 CSV files in the dataset\n",
    "files = [\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\"\n",
    "]\n",
    "\n",
    "# Dataset ID in BigQuery\n",
    "dataset_id = \"brazil_olist_data\"\n",
    "\n",
    "# Iterate through files, load into Pandas, and upload to BigQuery\n",
    "for file_path in files:\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    \n",
    "    # Load CSV into Pandas DataFrame\n",
    "    df = kagglehub.dataset_load(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"olistbr/brazilian-ecommerce\",\n",
    "        file_path\n",
    "    )\n",
    "    \n",
    "    # Define BigQuery table name (strip .csv)\n",
    "    table_id = f\"my-module-2-group-project.{dataset_id}.{file_path.replace('.csv','')}\"\n",
    "    \n",
    "    # Upload DataFrame to BigQuery\n",
    "    job = client.load_table_from_dataframe(df, table_id)\n",
    "    job.result()  # Wait for job to complete\n",
    "    \n",
    "    print(f\"✅ Uploaded {file_path} to {table_id}, shape: {df.shape}\")\n",
    "\n",
    "print(\"All files have been uploaded to BigQuery.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51874c",
   "metadata": {},
   "source": [
    "## Reusable ETL Function: Kaggle → Pandas → BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552ffa2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "def load_kaggle_to_bigquery(\n",
    "    dataset_name: str,\n",
    "    files: list,\n",
    "    dataset_id: str,\n",
    "    project_id: str,\n",
    "    service_account_path: str,\n",
    "    write_mode: str = \"WRITE_APPEND\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Load multiple Kaggle dataset files into BigQuery tables.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Kaggle dataset identifier, e.g. \"olistbr/brazilian-ecommerce\"\n",
    "        files (list): List of file names to load, e.g. [\"olist_orders_dataset.csv\", ...]\n",
    "        dataset_id (str): BigQuery dataset ID\n",
    "        project_id (str): GCP project ID\n",
    "        service_account_path (str): Path to service account JSON\n",
    "        write_mode (str): BigQuery write disposition (\"WRITE_APPEND\", \"WRITE_TRUNCATE\", \"WRITE_EMPTY\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Authenticate BigQuery client\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "    \n",
    "    for file_path in files:\n",
    "        print(f\"Loading {file_path} from Kaggle...\")\n",
    "        \n",
    "        # Load CSV into Pandas DataFrame\n",
    "        df = kagglehub.dataset_load(\n",
    "            KaggleDatasetAdapter.PANDAS,\n",
    "            dataset_name,\n",
    "            file_path\n",
    "        )\n",
    "        \n",
    "        # Define BigQuery table name (strip .csv)\n",
    "        table_id = f\"{project_id}.{dataset_id}.{file_path.replace('.csv','')}\"\n",
    "        \n",
    "        # Configure job\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_mode)\n",
    "        \n",
    "        # Upload DataFrame to BigQuery\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "        job.result()  # Wait for job to complete\n",
    "        \n",
    "        print(f\"✅ Uploaded {file_path} to {table_id}, shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92370785",
   "metadata": {},
   "source": [
    "## Example\n",
    "```py\n",
    "files = [\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"olist_geolocation_dataset.csv\",\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"product_category_name_translation.csv\"\n",
    "]\n",
    "\n",
    "load_kaggle_to_bigquery(\n",
    "    dataset_name=\"olistbr/brazilian-ecommerce\",\n",
    "    files=files,\n",
    "    dataset_id=\"ecommerce_dataset\",\n",
    "    project_id=\"your-gcp-project-id\",\n",
    "    service_account_path=\"path/to/service_account.json\",\n",
    "    write_mode=\"WRITE_APPEND\"   # or \"WRITE_TRUNCATE\" to overwrite\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
